{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import sys \n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b5a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a5eca8",
   "metadata": {},
   "source": [
    "## Enhance Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing\n",
    "in_path = 'coco/building-coco/256/original/images/samples/coco/images/val/'\n",
    "out_path = 'coco/building-coco/256/original/images/samples/coco/images/val_smoothing/'\n",
    "for afile in os.listdir(in_path):\n",
    "    im = cv2.imread(in_path+afile)\n",
    "    masked = cv2.bilateralFilter(im, 15, 75, 75)\n",
    "    cv2.imwrite(out_path+afile, masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33766fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge sharpening\n",
    "in_path = 'coco/building-coco/256/original/images/samples/coco/images/test/'\n",
    "out_path = 'coco/building-coco/256/original/images/samples/coco/images/test_sharpen/'\n",
    "for afile in os.listdir(in_path):\n",
    "    im = cv2.imread(in_path+afile)\n",
    "    kernel = np.array([[-1,-1,-1], \n",
    "                       [-1,9,-1], \n",
    "                       [-1,-1,-1]])\n",
    "    im = cv2.filter2D(im, -1, kernel)\n",
    "    cv2.imwrite(out_path+afile, im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcdd52",
   "metadata": {},
   "source": [
    "### Mask images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "from skimage import filters\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from segmentation_models_pytorch import utils\n",
    "import segmentation_models_pytorch.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'coco/building-coco/256/original/images/samples/coco/images/'\n",
    "\n",
    "random_crop_size = 256\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get class names\n",
    "class_names = ['background', 'building']\n",
    "# Get class RGB values\n",
    "class_rgb_values = [[0, 0, 0], [255, 255, 255]]\n",
    "select_classes = ['background', 'building']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d992e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class BuildingsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Massachusetts Buildings Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]            \n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # one-hot-encode the mask\n",
    "        #mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask, self.image_paths[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        #album.augmentations.transforms.Sharpen( alpha = (0.2, 0.5),lightness = (0.5, 1.0),always_apply = True,p = 0.5,),\n",
    "        album.RandomCrop(height=random_crop_size, width=random_crop_size, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        #album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "        album.PadIfNeeded(min_height=15, min_width=15, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    #x = adjust_sharpness(Image.fromarray(np.uint8(x)), 5)\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = class_names\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "if os.path.exists(os.path.join('./','best_model_128_unet.pth')):\n",
    "    best_model = torch.load(os.path.join('./','best_model_128_unet.pth'), map_location=DEVICE)\n",
    "    print('Loaded UNet model.')\n",
    "else:\n",
    "    print('fail to load model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = BuildingsDataset(\n",
    "    os.path.join('coco/building-coco/512','test'), #'../data/tiff_v3/128_modified_pre/'\n",
    "    os.path.join('coco/building-coco/512','test'),\n",
    "    # x_test_dir, \n",
    "    # y_test_dir, \n",
    "    # x_train_dir, \n",
    "    # y_train_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# test dataset for visualization (without preprocessing transformations)\n",
    "test_dataset_vis = BuildingsDataset(\n",
    "    os.path.join('coco/building-coco/512','test'),\n",
    "    os.path.join('coco/building-coco/512','test'),\n",
    "    # x_test_dir, \n",
    "    # y_test_dir, \n",
    "    # x_train_dir, \n",
    "    # y_train_dir, \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# get a random test image/mask index\n",
    "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
    "image, mask, img_path = test_dataset_vis[random_idx]\n",
    "\n",
    "# visualize(\n",
    "#     original_image = image,\n",
    "#     ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "#     one_hot_encoded_mask = reverse_one_hot(mask)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center crop padded image / mask to original image dims\n",
    "def crop_image(image, target_image_dims=[2500,2500,3]):\n",
    "   \n",
    "    target_size = target_image_dims[0]\n",
    "    image_size = len(image)\n",
    "    padding = (image_size - target_size) // 2\n",
    "\n",
    "    return image[\n",
    "        padding:image_size - padding,\n",
    "        padding:image_size - padding,\n",
    "        :,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds_folder = os.path.join('coco/building-coco/512/','tmp')\n",
    "if not os.path.exists(sample_preds_folder):\n",
    "    os.makedirs(sample_preds_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db13577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(len(test_dataset)):\n",
    "\n",
    "    image, gt_mask, img_path = test_dataset[idx]\n",
    "    image_vis = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n",
    "    gt_mask_vis = crop_image(test_dataset_vis[idx][1].astype('uint8'))\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    # Predict test image\n",
    "    pred_mask = best_model(x_tensor)\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to building\n",
    "    pred_building_heatmap = pred_mask[:,:,select_classes.index('building')]\n",
    "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    #gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "    #gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n",
    "    fname = img_path.split('/')[-1]\n",
    "    nimg = image_vis.copy()\n",
    "    nimg[pred_building_heatmap<0.02] = 0\n",
    "    # kernel = np.array([[-1,-1,-1], \n",
    "    #                    [-1,9,-1], \n",
    "    #                    [-1,-1,-1]])\n",
    "    # im = gt_mask_vis.copy()\n",
    "    # im = cv2.filter2D(im, -1, kernel)\n",
    "    \n",
    "    # gt_mask_vis[pred_building_heatmap>0.05] = im[pred_building_heatmap>0.05]\n",
    "#     cv2.imwrite(os.path.join(sample_preds_folder, f\"{img_path}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"{fname}\"), nimg)\n",
    "    \n",
    "    \n",
    "    visualize(\n",
    "        original_image = image_vis,\n",
    "        #ground_truth_mask = nimg,\n",
    "        predicted_mask = pred_mask,\n",
    "        selected_mask = nimg,\n",
    "        predicted_building_heatmap = pred_building_heatmap\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ae098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(pred_building_heatmap, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cac748",
   "metadata": {},
   "source": [
    "### Test YOLO building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee750572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "#in_path = 'coco/building-coco/256/original/images/samples/coco/images/val/'\n",
    "in_path = 'coco/building-coco/512/test_masked/'  #'coco/building-coco/512/test/'\n",
    "weights = './weights/yolov7-custom-128-512-masked/weights/best.pt' #'./weights/best_256_masked_sharpen.pt'\n",
    "filelist = sorted(os.listdir(in_path))\n",
    "\n",
    "for afile in filelist:\n",
    "    if len(afile.split('.')[0])>1:\n",
    "        comm = f'python detect.py  --save-txt --save-conf --weights {weights} --conf 0.25  --img-size 512 --source {in_path + afile}'\n",
    "        print(comm)\n",
    "        os.system(comm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3091e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = 'runs/detect/'\n",
    "labels = []\n",
    "for afolder in os.listdir(in_path):\n",
    "    if len(afolder.split('.')[0])>0:\n",
    "        file = os.listdir(in_path+afolder+'/labels')\n",
    "        if len(file)>0:\n",
    "            file = file[0]\n",
    "            with open(in_path+afolder+'/labels/'+file,'r') as f:\n",
    "                for aline in f.readlines():\n",
    "                    labels.append(aline[:-1].split(' '))\n",
    "            \n",
    "        else:\n",
    "            print('no labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50396950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np_labels = np.array(labels)\n",
    "plt.hist(np_labels[:,5], bins=10)\n",
    "np_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b35903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding according to the .yaml file class names order\n",
    "import os\n",
    "\n",
    "decoding_of_predictions ={0: 'undamagedcommercialbuilding', \\\n",
    "                          1: 'undamagedresidentialbuilding', \\\n",
    "                          2: 'damagedresidentialbuilding', \\\n",
    "                          3: 'damagedcommercialbuilding', \\\n",
    "                          4: 'background'}\n",
    "\n",
    "directory = 'challenge_1_submission_images/Validation_Data_JPEG'\n",
    "directory = 'runs/detect/'\n",
    "# Directory to store outputs\n",
    "results_directory = 'results/Validation_Data_Results'\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for folder in os.listdir(directory):\n",
    "    # Check if the current object is a file and ends with .jpeg\n",
    "    filename = ''\n",
    "    if len(folder.split('.')[0])>0:\n",
    "        filename = os.listdir(directory+folder+'/labels')\n",
    "    print(filename)\n",
    "    if len(filename)>0:\n",
    "        filename = filename[0]\n",
    "        if os.path.isfile(os.path.join(directory+folder+'/labels', filename)) and filename.lower().endswith('.txt'):\n",
    "            # Perform operations on the file\n",
    "            file_path = os.path.join(directory+folder+'/labels', filename)\n",
    "            print(file_path)\n",
    "            # print(\"Making a prediction on \", filename)\n",
    "            # results = model.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.25)\n",
    "            \n",
    "            # for r in results:\n",
    "            #     conf_list = r.boxes.conf.numpy().tolist()\n",
    "            #     clss_list = r.boxes.cls.numpy().tolist()\n",
    "            #     original_list = clss_list\n",
    "            #     updated_list = []\n",
    "            #     for element in original_list:\n",
    "            #          updated_list.append(decoding_of_predictions[int(element)])\n",
    "    \n",
    "            # bounding_boxes = r.boxes.xyxy.numpy()\n",
    "            # confidences = conf_list\n",
    "            # class_names = updated_list\n",
    "    \n",
    "            # # Check if bounding boxes, confidences and class names match\n",
    "            # if len(bounding_boxes) != len(confidences) or len(bounding_boxes) != len(class_names):\n",
    "            #     print(\"Error: Number of bounding boxes, confidences, and class names should be the same.\")\n",
    "            #     continue\n",
    "            with open(file_path,'r') as f:\n",
    "                bounding_boxes = f.readlines()\n",
    "                \n",
    "                bounding_boxes = [b[:-1].split(' ') for b in bounding_boxes]\n",
    "            text_file_name = os.path.splitext(filename)[0]\n",
    "            # Creating a new .txt file for each image in the submission_directory\n",
    "            with open(os.path.join(results_directory, f\"{text_file_name}.txt\"), \"w\") as file:\n",
    "                for i in range(len(bounding_boxes)):\n",
    "                    # Get coordinates of each bounding box\n",
    "                    class_name, confidence, left, top, right, bottom = bounding_boxes[i]\n",
    "                    # Write content to file in desired format\n",
    "                    file.write(f\"{decoding_of_predictions[int(class_name)]} {confidence} {left} {top} {right} {bottom}\\n\")\n",
    "            print(\"Output files generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your source directory and the destination where the zip file will be created\n",
    "source_dir = results_directory\n",
    "destination_zip = 'submission'\n",
    "\n",
    "# Create a zip file from the directory\n",
    "shutil.make_archive(destination_zip, 'zip', source_dir)\n",
    "\n",
    "print(f\"Directory {source_dir} has been successfully zipped into {destination_zip}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy masks to folder\n",
    "\n",
    "list_path = 'coco/building-coco/256/original/samples/YOLODataset/images/train/'\n",
    "in_path = 'coco/building-coco/256/masks_jpeg/'\n",
    "out_path = 'coco/building-coco/256/original/samples/YOLODataset/masks/train/'\n",
    "for aname in os.listdir(list_path):\n",
    "    \n",
    "    if aname.replace('.png','.jpg') in os.listdir(in_path):\n",
    "        \n",
    "        shutil.copy(in_path+aname.replace('.png','.jpg'), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce592c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask images\n",
    "\n",
    "list_path = 'coco/building-coco/256/original/samples/YOLODataset/'\n",
    "for aname in os.listdir(list_path + 'masks/val'):\n",
    "    img = cv2.imread(list_path + 'images/val/' + aname.replace('.jpg','.png'))\n",
    "    mask = cv2.imread(list_path + 'masks/val/' + aname)\n",
    "    \n",
    "    mask[mask<200] = 0\n",
    "    mask[mask>100] = 255\n",
    "    masked = img.copy()\n",
    "    masked[mask==0] = 0\n",
    "    cv2.imwrite(list_path + 'masked/val/' + aname,masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy labels to folder\n",
    "\n",
    "list_path = 'coco/building-coco/256/original/samples/YOLODataset/masked/images/val/'\n",
    "in_path = 'coco/building-coco/256/original/samples/YOLODataset/labels/val/'\n",
    "out_path = 'coco/building-coco/256/original/samples/YOLODataset/masked/labels/val/'\n",
    "for aname in os.listdir(list_path):\n",
    "    shutil.copy(in_path+aname.replace('.jpg','.txt'), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write train.txt, test.txt and val.txt\n",
    "\n",
    "list_path = 'coco/building-coco/256/original/samples//images/val/'\n",
    "with open('coco/building-coco/256/original/samples/val.txt','w') as f:\n",
    "    for aname in os.listdir(list_path):\n",
    "        f.write(aname+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09714216",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  './coco/plant-data/YelpChi.mat'\n",
    "\n",
    "yelp = loadmat(path)\n",
    "net_rur = yelp['net_rur']\n",
    "net_rtr = yelp['net_rtr']\n",
    "net_rsr = yelp['net_rsr']\n",
    "yelp_homo = yelp['homo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b742b93-7f4f-47fe-85e9-d7f0cf76e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in zip(yelp['homo'].todense(), yelp['label'][0]):\n",
    "    if item[1] >0:\n",
    "        print(item[0][item[0]!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4179731-946c-40f7-8a54-d72a041cb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['net_rur'].todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee9bc5-0b02-4cb9-83a4-5bbfa35af2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['label'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import pyvips\n",
    "# from imageio.v2 import imread #as imageio_imread, imsave\n",
    "# img = imread('coco/plant-coco/image_2016-01-10.tiff')\n",
    "# print(img.shape)\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = tifffile.imread('coco/plant-coco/image_2016-01-10.tiff')\n",
    "img_avg = np.average(np.array(img), axis=2)\n",
    "print(np.array(img_avg).shape)\n",
    "plt.imshow(img_avg)\n",
    "\n",
    "# for i in range(12):\n",
    "#     plt.imshow(np.array(img)[:,:,i])\n",
    "\n",
    "\n",
    "\n",
    "# def convert_tiff_to_jpeg_one(in_file):\n",
    "#     if in_file.endswith('.tiff'):\n",
    "#         x = pyvips.Image.new_from_file(in_file)\n",
    "        \n",
    "\n",
    "# in_file_path = 'coco/plant-coco/image_2016-01-10.tiff'\n",
    "# convert_tiff_to_jpeg_one(in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tiff_to_jpeg(input_dir,output_dir):\n",
    "    # check if output_dir exists, if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # check if file is an image (ends with .tif)\n",
    "        if filename.endswith('.tif'):\n",
    "            img = Image.open(os.path.join(input_dir, filename))\n",
    "        \n",
    "            # check if image is RGB mode, if not convert it\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "        \n",
    "            # create new filename, replace .tif with .jpg\n",
    "            output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        \n",
    "            # save the image in JPEG format\n",
    "            img.save(os.path.join(output_dir, output_filename), 'JPEG')\n",
    "    print(\"Conversion from TIFF to JPEG completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd07765",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = 'coco/building-coco/128/test_labels'\n",
    "out_path = 'coco/building-coco/128/test_labels_jpeg'\n",
    "convert_tiff_to_jpeg(in_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile images to make sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9924c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1]*2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "def crop_one_box(x, img):\n",
    "    # Plots one bounding box on image img\n",
    "    gn = torch.tensor(img.shape)[[1, 0, 1, 0]]\n",
    "    x = (x[:4]*gn).view(-1).tolist()\n",
    "    xy, wh = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    return img[(xy[1]-wh[1]//2):(xy[1]+wh[1]//2),(xy[0]-wh[0]//2):(xy[0]+wh[0]//2)]\n",
    "    \n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278eb83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tile YOLO Exp\n",
    "\n",
    "path = '/Users/lucy/workspace/development/Chronic Disease/source/imaging/building-data/samples/512_v3_original_0.02/'\n",
    "image_path = '/Users/lucy/workspace/development/Chronic Disease/source/imaging/building-data/samples/512/test/'\n",
    "\n",
    "folderlist = os.listdir(path)\n",
    "count = 0\n",
    "for folder in folderlist:\n",
    "    if len(folder.split('.')[0])>0:\n",
    "        print(folder)\n",
    "        try:\n",
    "            filename = os.listdir(path + folder+'/labels')\n",
    "            if len(filename)>0:\n",
    "                filename = filename[0]\n",
    "                print(filename)\n",
    "                \n",
    "                if len(filename)>0:\n",
    "                    if os.path.exists(image_path+'/'+filename.replace('txt','png')):\n",
    "                        img = cv2.imread(image_path+'/'+filename.replace('txt','png'))\n",
    "                    elif os.path.exists(image_path+'/'+filename.replace('txt','jpg')):\n",
    "                        img = cv2.imread(image_path+'/'+filename.replace('txt','jpg'))\n",
    "                    else:\n",
    "                        break\n",
    "                    #print(img.shape)\n",
    "                    with open(path+folder+'/labels/'+filename,'r') as flabel:\n",
    "                        labels = flabel.readlines()\n",
    "                        lcount = 0\n",
    "                        for alabel in labels:\n",
    "                            lcount +=1\n",
    "                            alabel = torch.FloatTensor([float(a) for a in alabel[:-1].split(' ')])\n",
    "                            \n",
    "                            atile = crop_one_box(alabel[2:6], img)\n",
    "\n",
    "                            if atile.shape[0]>0 and atile.shape[1]>0:\n",
    "                                t_path = path+folder+'/'+filename.replace('.txt','')+'_'+str(lcount)+'.png'\n",
    "                                print(t_path)\n",
    "                                cv2.imwrite(t_path,atile)\n",
    "                                # t_path = path+folder+'/'+filename.replace('.txt','')+'_boxed'+str(lcount)+'.png'\n",
    "                                # #print(t_path)\n",
    "                                # cv2.imwrite(t_path,aboxed)\n",
    "            \n",
    "        except:\n",
    "            print(path + folder+'/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d79f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d92db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ee7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1]*2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "def crop_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    gn = torch.tensor(img.shape)[[1, 0, 1, 0]]\n",
    "    x = (x[:4]*gn).view(-1).tolist()\n",
    "    xy, wh = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    return img[(xy[1]-wh[1]//2):(xy[1]+wh[1]//2),(xy[0]-wh[0]//2):(xy[0]+wh[0]//2)]\n",
    "    \n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "    \n",
    "      \n",
    "path =        '/Users/lucy/workspace/development/Chronic Disease/source/imaging/building-data/samples/512_v3_original_0.02/exp/' \n",
    "aimage = cv2.imread(path + 'Validation_Post_Event_001.jpg')\n",
    "gn = torch.tensor(aimage.shape)[[1, 0, 1, 0]]\n",
    "\n",
    "tl = round(0.002 * (aimage.shape[0] + aimage.shape[1]) / 2) + 1\n",
    "tf = max(tl-1, 1)\n",
    "\n",
    "color = [random.randint(0, 255) for _ in range(3)]\n",
    "\n",
    "xx = torch.Tensor([0.25, 0.547852, 0.0585938, 0.0566406,0.5])\n",
    "xyxy = (xywh2xyxy(xx[:-1].view(1,-1))*gn).view(-1).tolist()\n",
    "xyxy.append(0.5)\n",
    "\n",
    "\n",
    "\n",
    "aimage_copy = aimage.copy()\n",
    "newim = crop_one_box(xx[:4], aimage_copy)\n",
    "\n",
    "#plot_one_box(xyxy, aimage) \n",
    "\n",
    "newim_labeled = crop_one_box(xx[:4], aimage)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite('./inference/images/test.jpg', aimage)\n",
    "plt.imshow(newim)\n",
    "plt.show()\n",
    "plt.imshow(newim_labeled)\n",
    "plt.show()\n",
    "plt.imshow(aimage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298e614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80518a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    \n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    #if label:\n",
    "    label = 'B '+ str(x[-1])\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1]*2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "def crop_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    gn = torch.tensor(img.shape)[[1, 0, 1, 0]]\n",
    "    x = (x[:4]*gn).view(-1).tolist()\n",
    "    xy, wh = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    return img[(xy[1]-wh[1]//2):(xy[1]+wh[1]//2),(xy[0]-wh[0]//2):(xy[0]+wh[0]//2)]\n",
    "    \n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "    \n",
    "      \n",
    "        \n",
    "aimage = cv2.imread('inference/images/bus.jpg')\n",
    "gn = torch.tensor(aimage.shape)[[1, 0, 1, 0]]\n",
    "\n",
    "tl = round(0.002 * (aimage.shape[0] + aimage.shape[1]) / 2) + 1\n",
    "tf = max(tl-1, 1)\n",
    "\n",
    "color = [random.randint(0, 255) for _ in range(3)]\n",
    "\n",
    "xx = torch.Tensor([0.182716, 0.603241, 0.246914, 0.467593, 0.5])\n",
    "xyxy = (xywh2xyxy(xx[:-1].view(1,-1))*gn).view(-1).tolist()\n",
    "xyxy.append(0.5)\n",
    "\n",
    "aimage_copy = aimage.copy()\n",
    "newim = crop_one_box(xx[:4], aimage_copy)\n",
    "\n",
    "plot_one_box(xyxy, aimage) \n",
    "\n",
    "newim_labeled = crop_one_box(xx[:4], aimage)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite('./inference/images/test.jpg', aimage)\n",
    "plt.imshow(newim)\n",
    "plt.show()\n",
    "plt.imshow(newim_labeled)\n",
    "plt.show()\n",
    "plt.imshow(aimage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py --workers 8 --device 0 --batch-size 4 --data data/building-256-coco.yaml --img 256 256 --cfg cfg/training/yolov7.yaml --name yolov7 --hyp data/hyp.scratch.custom.yaml\n",
    "\n",
    "\n",
    "# python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source yourvideo.mp4\n",
    "comm = 'python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --save-txt --save-conf --source ./inference/images/bus.jpg'\n",
    "\n",
    "os.system(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba12627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size[0] - img.size[0]\n",
    "    delta_height = desired_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceefcc2-0e0b-483e-b661-b4e9816f68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['t_comm','t_dcomm']\n",
    "in_path = '../../building-data/samples/training_v18/'\n",
    "out_path = '../../building-data/samples/training_v18/resized/'\n",
    "for afolder in folders:\n",
    "        \n",
    "    for afile in os.listdir(in_path+afolder):\n",
    "        if len(afile.split('.')[0])>0:\n",
    "            img = cv2.imread(in_path+afolder+'/'+afile)\n",
    "            kernel = np.array([[-1,-1,-1], \n",
    "                       [-1, 9,-1],\n",
    "                       [-1,-1,-1]])\n",
    "            img = cv2.filter2D(img, -1, kernel)\n",
    "            img = padding(Image.fromarray(img), [256,256])\n",
    "            img.save(out_path+afolder+'/'+afile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc635816-258e-4632-b445-6c017309fb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "in_path = '../../building-data/samples/512_v2_sharpen_0.02/'\n",
    "#out_path = '../../building-data/samples/v2_128post/resized/res_dmg/'\n",
    "for i in range(1,10):\n",
    "    var = '0'+str(i)\n",
    "    for foldername in os.listdir(in_path+var):\n",
    "        if len(foldername.split('.')[0])>0:\n",
    "            folderlist = os.listdir(in_path+var+'/'+foldername)\n",
    "            for afile in folderlist:\n",
    "                shutil.copy(in_path+var+'/'+foldername+'/'+afile, in_path+foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ab1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image sharpening\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('inference/images/bus.jpg')\n",
    "kernel = np.array([[-1,-1,-1], \n",
    "                   [-1, 9,-1],\n",
    "                   [-1,-1,-1]])\n",
    "sharpened = cv2.filter2D(image, -1, kernel) # applying the sharpening kernel to the input image & displaying it.\n",
    "cv2.imshow('Image Sharpening', sharpened)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Image Sharpening')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa325df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image masking\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('../../building-data/128/pre/selected_jpeg_backup/tile_0_357.jpg')\n",
    "mask = cv2.imread('../../building-data/128/building/selected_backup_jpeg/tile_0_357.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8562b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask<200]=0\n",
    "mask[mask>100]=255\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616603fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.average(mask, axis=-1)\n",
    "mask = mask.astype(np.float32)\n",
    "mask = cv2.merge([mask, mask, mask])\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320395ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv2.imshow(\"Rectangular Mask\", mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Rectangular Mask')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad637b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masked = image*mask*255 #np.clip(image+mask, 0, 1) #\n",
    "masked = cv2.bilateralFilter(masked, 15, 75, 75) \n",
    "nimage = image.copy()\n",
    "nimage[mask!=0] = masked[mask!=0]\n",
    "masked[mask!=0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12316f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv2.imshow(\"Rectangular Mask\", mask)\n",
    "cv2.imshow(\"Mask Applied to Image\", masked)\n",
    "cv2.imshow(\"Masked Image\", nimage)\n",
    "cv2.imshow('Original Image', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image masking\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('inference/images/bus.jpg')\n",
    "\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "cv2.rectangle(mask, (0, 90), (290, 450), 255, -1)\n",
    "cv2.imshow(\"Rectangular Mask\", mask)\n",
    "\n",
    "\n",
    "masked = cv2.bitwise_and(image, image, mask=mask)\n",
    "cv2.imshow(\"Mask Applied to Image\", new_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow(\"Mask Applied to Image\")\n",
    "cv2.destroyWindow('Rectangular Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ab1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mask to region\n",
    "mask = np.ones([5,5])\n",
    "region = np.ones([5,5])\n",
    "region[1:3, 2:3] = 0\n",
    "mask[1:3,2:3 ] = region[1:3, 2:3]\n",
    "mask, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing averaging blurring on our Cybertruck image\n",
    "# Filters - left (3,3), middle(5,5), right(9,9)\n",
    "blurred_1 = np.hstack([\n",
    "  cv2.blur(img1,(3,3)),\n",
    "  cv2.blur(img1,(5,5)),\n",
    "  cv2.blur(img1,(9,9))])  \n",
    "cv2_imshow(blurred_1)\n",
    "\n",
    "# Performing averaging blurring on our Logo image\n",
    "# Filters - left (3,3), middle(5,5), right(9,9)\n",
    "blurred_2 = np.hstack([\n",
    "  cv2.blur(img2,(3,3)),\n",
    "  cv2.blur(img2,(5,5)),\n",
    "  cv2.blur(img2,(9,9))])  \n",
    "cv2_imshow(blurred_2)\n",
    "\n",
    "# Performing Gaussian blurring on our Cybertruck image\n",
    "Gaussian_blurred_1 = np.hstack([\n",
    "  cv2.GaussianBlur(img1,(3,3),0),\n",
    "  cv2.GaussianBlur(img1,(5,5),0),\n",
    "  cv2.GaussianBlur(img1,(9,9),0)])  \n",
    "cv2_imshow(Gaussian_blurred_1)\n",
    "\n",
    "# Performing Gausssian blurring on our Logo image\n",
    "Gaussian_blurred_2 = np.hstack([\n",
    "  cv2.GaussianBlur(img2,(3,3),0),\n",
    "  cv2.GaussianBlur(img2,(5,5),0),\n",
    "  cv2.GaussianBlur(img2,(9,9),0)])  \n",
    "cv2_imshow(Gaussian_blurred_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "from skimage import filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from segmentation_models_pytorch import utils\n",
    "import segmentation_models_pytorch.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = 'coco/building-coco/128/test/'\n",
    "out_path = 'coco/building-coco/'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = torch.load(os.path.join('./','best_model_256.pth'), map_location=DEVICE)\n",
    "\n",
    "for aimage in os.listdir(in_path):\n",
    "\n",
    "    image = cv2.imread(in_path+aimage)\n",
    "    \n",
    "    x_tensor = torch.FloatTensor(np.transpose(image,(2, 0, 1))).unsqueeze(0)\n",
    "    \n",
    "    # Predict test image\n",
    "    pred_mask = best_model(x_tensor).squeeze().detach().numpy()\n",
    "\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    pred_mask = np.round(np.average(pred_mask, axis=-1), decimals =0)\n",
    "    pred_mask = cv2.merge([pred_mask, pred_mask, pred_mask])\n",
    "    nimage = image.copy()\n",
    "    nimage[pred_mask==0] = pred_mask[pred_mask==0]\n",
    "    cv2.imshow('original image',image)\n",
    "    cv2.imshow('mask',pred_mask)\n",
    "    cv2.imshow('masked image',nimage)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    break\n",
    "    \n",
    "    \n",
    "    cv2.imwrite(os.path.join(out_path+'masked', aimage.replace('.tif','.png')), pred_mask)\n",
    "    cv2.imwrite(os.path.join(out_path+'images', aimage.replace('.tif','.png')), image)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "in_path = 'coco/building-coco/classification/all_valid/'\n",
    "out_path = 'coco/building-coco/classification/'\n",
    "folderlist = os.listdir(in_path)\n",
    "for afolder in folderlist: # folder 001 - 012\n",
    "    cfolderlist = os.listdir(in_path+afolder)\n",
    "    cfolderlist = [f for f in cfolderlist if os.path.isdir(f)]\n",
    "    for cfolder in cfolderlist: # class folders\n",
    "        afilelist = os.listdir(in_path+afolder+'/'+cfolder)\n",
    "        afilelist = [f for f in afilelist if f.split('.')[-1]=='png']\n",
    "        for afile in afilelist: # images\n",
    "            shutil.copy(in_path+afolder+'/'+cfolder+'/'+afile, out_path+cfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f161832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding according to the .yaml file class names order\n",
    "decoding_of_predictions ={0: 'undamagedcommercialbuilding', 1: 'undamagedresidentialbuilding', 2: 'damagedresidentialbuilding', 3: 'damagedcommercialbuilding'}\n",
    "#decoding_of_predictions ={3: 'undamagedcommercialbuilding', 2: 'undamagedresidentialbuilding', 1: 'damagedresidentialbuilding', 0: 'damagedcommercialbuilding'}\n",
    "\n",
    "directory = 'results/challenge_1_submission_images/Validation_Data_JPEG/'\n",
    "# Directory to store outputs\n",
    "results_directory = 'results/Validation_Data_Results/'\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for afolder in os.listdir(directory):\n",
    "    # Check if the current object is a file and ends with .jpeg\n",
    "    if len(afolder.split('.')[0])>0:\n",
    "        labels = os.listdir(directory+afolder+'/labels/')\n",
    "        if len(labels)>0:\n",
    "            filename = labels[0]\n",
    "            with open(os.path.join(directory+afolder+'/labels', filename), 'r') as f:\n",
    "                \n",
    "                text_file_name = os.path.splitext(filename)[0]\n",
    "                # Creating a new .txt file for each image in the submission_directory\n",
    "                with open(os.path.join(results_directory, f\"{text_file_name}.txt\"), \"w\") as file:\n",
    "                    \n",
    "                    for aline in f.readlines():\n",
    "                        alabel = aline[:-1].split(' ')\n",
    "                        \n",
    "                        bounding_boxes = alabel[2:]\n",
    "                        confidence = alabel[1]\n",
    "                        class_name = int(alabel[0])\n",
    "                        # Get coordinates of each bounding box\n",
    "                        left, top, right, bottom = bounding_boxes\n",
    "                        # Write content to file in desired format\n",
    "                        file.write(f\"{decoding_of_predictions[class_name]} {confidence} {left} {top} {right} {bottom}\\n\")\n",
    "                    print(\"Output files generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
